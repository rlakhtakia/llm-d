apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: wide-ep-llm-d-prefill
  labels:
    llm-d.ai/inference-serving: "true"
    llm-d.ai/guide: "wide-ep-lws"
    llm-d.ai/accelerator-variant: "gpu"
    llm-d.ai/accelerator-vendor: "nvidia"
    llm-d.ai/model: DeepSeek-R1-0528
    llm-d.ai/role: prefill
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    workerTemplate:
      metadata:
        labels:
          llm-d.ai/inference-serving: "true"
          llm-d.ai/guide: "wide-ep-lws"
          llm-d.ai/accelerator-variant: "gpu"
          llm-d.ai/accelerator-vendor: "nvidia"
          llm-d.ai/model: DeepSeek-R1-0528
          llm-d.ai/role: prefill
      spec:
        serviceAccountName: deepseek-r1
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 3Gi  # roughly 32MB per local DP plus scratch space
          - name: hf-cache
            emptyDir: {}
          - name: jit-cache
            emptyDir: {}
        containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
            securityContext:
              capabilities:
                add:
                  - IPC_LOCK
                  - SYS_RAWIO
              runAsGroup: 0
              runAsUser: 0
            imagePullPolicy: Always
            command:
              - /bin/bash
              - -c
            args:
              - |-
                # Clear /dev/shm on start to prevent running out of space when crashes occur
                # https://github.com/llm-d/llm-d/issues/352
                find /dev/shm -type f -delete

                #################
                # RUN vLLM prefill workers with DP-aware scheduling
                #################

                # --data-parallel-hybrid-lb: Use exernal load balancing across nodes, and internal load balancing within a node
                # --enable-expert-parallel: Use TPxDP in attention, EP in MoE layers
                # --async-scheduling: Reduce white space between engine steps
                # --enable-dbo: Dual batch overlap (DBO) overlaps compute with collective communication.
                # --enable-eplb:  Expert-parallel load balancing reduces EP load imbalance by replicating heavily-used experts

                START_RANK=$(( ${LWS_WORKER_INDEX:-0} * 8 ))

                # Common vLLM flag arguments shared across all decode ranks
                COMMON_VLLM_ARGS=(
                  --max-model-len 65536
                  --disable-uvicorn-access-log
                  --tensor-parallel-size=1
                  --data-parallel-external-lb
                  --data-parallel-size=16
                  --data-parallel-address="${LWS_LEADER_ADDRESS}"
                  --data-parallel-rpc-port=5555
                  --node-rank="${LWS_WORKER_INDEX:-0}"
                  --enable-expert-parallel
                  --async-scheduling
                  --enable-dbo
                  --dbo-prefill-token-threshold 32
                  --enable-eplb
                  --eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}'
                  --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}'
                  --all2all-backend deepep_high_throughput
                  --gpu-memory-utilization 0.75
                )

                for i in {0..7}; do
                  CUDA_VISIBLE_DEVICES="$i" vllm serve deepseek-ai/DeepSeek-R1-0528 \
                    --host=0.0.0.0 --port=$((8000 + i)) \
                    --data-parallel-rank=$((START_RANK + i)) "${COMMON_VLLM_ARGS[@]}" &
                done
                wait
              - vllm-parallel-launcher
            env:
              - name: DP_SIZE_LOCAL
                value: "8"
              - name: TP_SIZE
                value: "1"
              - name: TRITON_LIBCUDA_PATH
                value: /usr/lib64
              - name: VLLM_SKIP_P2P_CHECK
                value: "1"
              - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
                value: "1"
              - name: VLLM_USE_DEEP_GEMM
                value: "1"
              - name: VLLM_ALL2ALL_BACKEND
                value: deepep_high_throughput
              - name: NVIDIA_GDRCOPY
                value: enabled
              - name: NVSHMEM_REMOTE_TRANSPORT
                value: ibgda
              - name: NVSHMEM_IB_ENABLE_IBGDA
                value: "true"
              - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                value: eth0
              - name: GLOO_SOCKET_IFNAME
                value: eth0
              - name: NCCL_SOCKET_IFNAME
                value: eth0
              - name: VLLM_LOGGING_LEVEL
                value: INFO
              - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                valueFrom:
                  fieldRef:
                    fieldPath: status.podIP

              # Use cache directories from the mounted volume under one easy to mount
              # root directory.
              - name: CUDA_CACHE_PATH
                value: /var/cache/vllm/cuda
              - name: CCACHE_DIR
                value: /var/cache/vllm/ccache
              - name: VLLM_CACHE_ROOT
                value: /var/cache/vllm/vllm
              - name: FLASHINFER_WORKSPACE_BASE
                value: /var/cache/vllm/flashinfer
              # HuggingFace is likely to be quite a bit larger, give it its own cache
              - name: HF_HUB_CACHE
                value: /var/cache/huggingface
      

              # DP-aware scheduling: NCCL configuration
              - name: TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC
                value: "60"
              - name: TORCH_NCCL_ENABLE_MONITORING
                value: "0"
              - name: PYTORCH_ALLOC_CONF
                value: "expandable_segments:True" 

            ports:
              - containerPort: 5555
                name: dprpc
                protocol: TCP
              - containerPort: 5557
                name: nixl
                protocol: TCP
              - containerPort: 8000
                name: rank0
                protocol: TCP
              - containerPort: 8001
                name: rank1
                protocol: TCP
              - containerPort: 8002
                name: rank2
                protocol: TCP
              - containerPort: 8003
                name: rank3
                protocol: TCP
              - containerPort: 8004
                name: rank4
                protocol: TCP
              - containerPort: 8005
                name: rank5
                protocol: TCP
              - containerPort: 8006
                name: rank6
                protocol: TCP
              - containerPort: 8007
                name: rank7
                protocol: TCP
            startupProbe:
              httpGet:
                path: /health
                port: rank0
              initialDelaySeconds: 0
              periodSeconds: 1
              timeoutSeconds: 5
              failureThreshold: 2700
            livenessProbe:
              httpGet:
                path: /health
                port: rank0
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/models
                port: rank0
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            resources:
              limits:
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "8"
              requests:
                cpu: 32
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "8"
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
              - name: hf-cache
                mountPath: /var/cache/huggingface
              - name: jit-cache
                mountPath: /var/cache/vllm
