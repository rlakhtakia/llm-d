apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: wide-ep-llm-d-decode
  labels:
    llm-d.ai/inference-serving: "true"
    llm-d.ai/guide: "wide-ep-lws"
    llm-d.ai/accelerator-variant: "gpu"
    llm-d.ai/accelerator-vendor: "nvidia"
    llm-d.ai/model: DeepSeek-R1-0528
    llm-d.ai/role: decode
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    workerTemplate:
      metadata:
        labels:
          llm-d.ai/inference-serving: "true"
          llm-d.ai/guide: "wide-ep-lws"
          llm-d.ai/accelerator-variant: "gpu"
          llm-d.ai/accelerator-vendor: "nvidia"
          llm-d.ai/model: DeepSeek-R1-0528
          llm-d.ai/role: decode
      spec:
        serviceAccountName: deepseek-r1
        initContainers:
          - name: routing-proxy
            args:
              - --port=8000
              - --vllm-port=8200
              - --connector=nixlv2
              - --zap-log-level=4
              - --secure-proxy=false
              - --enable-prefiller-sampling
            image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
            imagePullPolicy: Always
            ports:
              - containerPort: 8000
                name: rank0
                protocol: TCP
              - containerPort: 8001
                name: rank1
                protocol: TCP
              - containerPort: 8002
                name: rank2
                protocol: TCP
              - containerPort: 8003
                name: rank3
                protocol: TCP
              - containerPort: 8004
                name: rank4
                protocol: TCP
              - containerPort: 8005
                name: rank5
                protocol: TCP
              - containerPort: 8006
                name: rank6
                protocol: TCP
              - containerPort: 8007
                name: rank7
                protocol: TCP
            resources: {}
            restartPolicy: Always
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 3Gi  # roughly 32MB per local DP plus scratch space
          - name: hf-cache
            emptyDir: {}
          - name: jit-cache
            emptyDir: {}
        containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
            securityContext:
              capabilities:
                add:
                  - IPC_LOCK
                  - SYS_RAWIO
              runAsGroup: 0
              runAsUser: 0
            imagePullPolicy: Always
            command:
              - /bin/bash
              - -c
            args:
              - |-
                # Clear /dev/shm on start to prevent running out of space when crashes occur
                # https://github.com/llm-d/llm-d/issues/352
                find /dev/shm -type f -delete

                #################
                # RUN vLLM decode workers with DP-aware scheduling
                #################
                # Launch 2 separate vLLM instances (one per GPU) with explicit DP ranks
                # --enable-expert-parallel:  Use TPxDP in attention, EP in MoE layers
                # --async-scheduling: Reduce white space between engine steps
                # --enable-dbo:  Dual batch overlap (DBO) overlaps compute with collective communication.
                # --enable-eplb: Expert-parallel load balancing reduces EP load imbalance by replicating heavily-used experts

                START_RANK=$(( ${LWS_WORKER_INDEX:-0} * 8 ))

                # Common vLLM flag arguments shared across all decode ranks
                COMMON_VLLM_ARGS=(
                  --max-model-len 65536
                  --disable-uvicorn-access-log
                  --tensor-parallel-size=1
                  --data-parallel-external-lb
                  --data-parallel-size=16
                  --data-parallel-address="${LWS_LEADER_ADDRESS}"
                  --data-parallel-rpc-port=5555
                  --node-rank=${LWS_WORKER_INDEX:-0}
                  --enable-expert-parallel
                  --async-scheduling
                  --enable-dbo
                  --dbo-decode-token-threshold 32
                  --enable-eplb
                  --eplb-config '{"window_size":"1000","step_interval":"3000","num_redundant_experts":"32","log_balancedness":"False"}'
                  --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both","kv_load_failure_policy":"fail"}'
                  --compilation_config '{"cudagraph_mode":"FULL_DECODE_ONLY"}'
                  --all2all-backend deepep_low_latency
                  --max-num-batched-tokens="${VLLM_MOE_DP_CHUNK_SIZE:-256}"
                )

                if [ ! -z "${KV_CACHE_MEMORY_BYTES-}" ]; then
                  COMMON_VLLM_ARGS+=( "--kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES}" )
                fi

                for i in {0..7}; do
                  CUDA_VISIBLE_DEVICES="$i" vllm serve deepseek-ai/DeepSeek-R1-0528 \
                    --host=0.0.0.0 --port=$((8200 + i)) \
                    --data-parallel-rank=$((START_RANK + i)) "${COMMON_VLLM_ARGS[@]}" &
                done
              - vllm-parallel-launcher
            env:
              # VLLM_MOE_DP_CHUNK_SIZE is a tunable parameter.
              # Higher values increase the memory footprint of the hidden states in the MoE layers,
              # which decreases the available memory for KV Cache.
              # Lower values may cause poor performance, especially in load-imbalanced scenarios.
              # The value 384 was chosen to be greater than the concurrency expected to see on any DP rank.
              # If you increase the VLLM_MOE_DP_CHUNK_SIZE above 510 you will also need to increase the
              # default NVSHMEM_QP_DEPTH to be at least 2 * (chunk_size + 1), which will increase the
              # memory NVSHMEM allocates up front.
              # Has no effect when VLLM_ALL2ALL_BACKEND=deepep_high_throughput.
              - name: VLLM_MOE_DP_CHUNK_SIZE
                value: "384" # vLLM default is 256
              - name: DP_SIZE_LOCAL
                value: "8"
              - name: TP_SIZE
                value: "1"
              - name: TRITON_LIBCUDA_PATH
                value: /usr/lib64
              - name: VLLM_SKIP_P2P_CHECK
                value: "1"
              - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
                value: "1"
              - name: VLLM_USE_DEEP_GEMM
                value: "1"
              - name: VLLM_ALL2ALL_BACKEND
                value: deepep_low_latency
              - name: NVIDIA_GDRCOPY
                value: enabled
              - name: NVSHMEM_REMOTE_TRANSPORT
                value: ibgda
              - name: NVSHMEM_IB_ENABLE_IBGDA
                value: "true"
              - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                value: eth0
              - name: GLOO_SOCKET_IFNAME
                value: eth0
              - name: NCCL_SOCKET_IFNAME
                value: eth0
              - name: VLLM_LOGGING_LEVEL
                value: INFO
              - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                valueFrom:
                  fieldRef:
                    fieldPath: status.podIP

              # Use cache directories from the mounted volume under one easy to mount
              # root directory.
              - name: CUDA_CACHE_PATH
                value: /var/cache/vllm/cuda
              - name: CCACHE_DIR
                value: /var/cache/vllm/ccache
              - name: VLLM_CACHE_ROOT
                value: /var/cache/vllm/vllm
              - name: FLASHINFER_WORKSPACE_BASE
                value: /var/cache/vllm/flashinfer
              # HuggingFace is likely to be quite a bit larger, give it its own cache
              - name: HF_HUB_CACHE
                value: /var/cache/huggingface

              # DP-aware scheduling: NCCL configuration
              - name: TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC
                value: "60"
              - name: TORCH_NCCL_ENABLE_MONITORING
                value: "0"
              - name: PYTORCH_ALLOC_CONF
                value: "expandable_segments:True" 

            ports:
              - containerPort: 5555
                name: dprpc
                protocol: TCP
              - containerPort: 5557
                name: nixl
                protocol: TCP
              - containerPort: 8200
                name: rank0
                protocol: TCP
              - containerPort: 8201
                name: rank1
                protocol: TCP
              - containerPort: 8202
                name: rank2
                protocol: TCP
              - containerPort: 8203
                name: rank3
                protocol: TCP
              - containerPort: 8204
                name: rank4
                protocol: TCP
              - containerPort: 8205
                name: rank5
                protocol: TCP
              - containerPort: 8206
                name: rank6
                protocol: TCP
              - containerPort: 8207
                name: rank7
                protocol: TCP
            startupProbe:
              httpGet:
                path: /health
                port: rank0
              initialDelaySeconds: 0
              periodSeconds: 1
              timeoutSeconds: 5
              failureThreshold: 2700
            livenessProbe:
              httpGet:
                path: /health
                port: rank0
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/models
                port: rank0
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            resources:
              limits:
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "8"
              requests:
                cpu: 32
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "8"
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
              - name: hf-cache
                mountPath: /var/cache/huggingface
              - name: jit-cache
                mountPath: /var/cache/vllm
